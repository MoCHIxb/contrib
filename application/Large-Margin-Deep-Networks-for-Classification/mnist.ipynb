{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19776:15908,MainProcess):2025-04-01-15:34:35.479.000 [mindspore\\context.py:1335] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.297429\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.624381\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.411722\n",
      "Epoch 1 completed, Average Loss: 0.808527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19776:15908,MainProcess):2025-04-01-15:35:03.479.000 [mindspore\\common\\api.py:114] The function \"train_step\" at the file \"C:\\Users\\高晨曦\\AppData\\Local\\Temp\\ipykernel_19776\\2269107119.py\", line 76 has been compiled again. Try to decorate the function with @jit(hash_args=...) or @jit(compile_once=True) to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.37%\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.347569\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.268205\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.265951\n",
      "Epoch 1 completed, Average Loss: 0.291046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19776:15908,MainProcess):2025-04-01-15:35:30.318.000 [mindspore\\common\\api.py:114] The function \"train_step\" at the file \"C:\\Users\\高晨曦\\AppData\\Local\\Temp\\ipykernel_19776\\2269107119.py\", line 76 has been compiled again. Try to decorate the function with @jit(hash_args=...) or @jit(compile_once=True) to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.50%\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.207034\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.259571\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.154492\n",
      "Epoch 1 completed, Average Loss: 0.199898\n",
      "Test Accuracy: 94.63%\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.182980\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.196487\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.160721\n",
      "Epoch 1 completed, Average Loss: 0.157663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19776:15908,MainProcess):2025-04-01-15:36:21.497.000 [mindspore\\common\\api.py:114] The function \"train_step\" at the file \"C:\\Users\\高晨曦\\AppData\\Local\\Temp\\ipykernel_19776\\2269107119.py\", line 76 has been compiled again. Try to decorate the function with @jit(hash_args=...) or @jit(compile_once=True) to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.13%\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.095938\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.127688\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.101721\n",
      "Epoch 1 completed, Average Loss: 0.128788\n",
      "Test Accuracy: 95.95%\n"
     ]
    }
   ],
   "source": [
    "# 设置设备\n",
    "\n",
    "\n",
    "# 导入必要的模块\n",
    "import os\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor, jit\n",
    "from mindspore.dataset import MnistDataset\n",
    "from mindspore.dataset.transforms import TypeCast\n",
    "from mindspore.dataset.vision import Rescale, HWC2CHW, RandomCrop, RandomHorizontalFlip\n",
    "from mindspore import context\n",
    "\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")\n",
    "# 数据加载函数\n",
    "def create_dataset(data_path, usage=\"train\", batch_size=256, num_workers=4):\n",
    "    mnist_dataset = MnistDataset(dataset_dir=data_path, usage=usage, shuffle=True)\n",
    "\n",
    "    transform = [\n",
    "        Rescale(1.0 / 127.5, -1),  # 归一化到 [-1, 1]\n",
    "        RandomCrop(28, padding=4),  # 随机裁剪\n",
    "        RandomHorizontalFlip(prob=0.5),  # 随机水平翻转\n",
    "        HWC2CHW()  # 确保通道在前 (HWC -> CHW)\n",
    "    ]\n",
    "\n",
    "    mnist_dataset = mnist_dataset.map(transform, 'image')\n",
    "    mnist_dataset = mnist_dataset.map(TypeCast(ms.float32), 'image')  # 图像数据转为 float32\n",
    "    mnist_dataset = mnist_dataset.map(TypeCast(ms.int32), 'label')   # 标签数据转为 int32\n",
    "\n",
    "    mnist_dataset = mnist_dataset.batch(batch_size, drop_remainder=True)  # 按批量分组\n",
    "\n",
    "    return mnist_dataset\n",
    "\n",
    "\n",
    "# 定义网络\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, pad_mode='valid')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Dense(64 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Dense(512, 128)\n",
    "        self.fc3 = nn.Dense(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.shape[0], -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits, [logits]\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train(net, loss_fn, optimizer, train_dataset, epoch_size=5):\n",
    "    total_train_samples = 60000\n",
    "    batch_size = 256\n",
    "    batches_per_epoch = total_train_samples // batch_size\n",
    "\n",
    "    @jit(hash_args=lambda data, label: (data.shape, label.shape), compile_once=True)\n",
    "    def forward_fn(data, label):\n",
    "        logits, _ = net(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "    @jit(hash_args=lambda data, label: (data.shape, label.shape), compile_once=True)\n",
    "    def train_step(data, label):\n",
    "        loss, grads = grad_fn(data, label)\n",
    "        optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "    try:\n",
    "        for epoch in range(epoch_size):\n",
    "            epoch_loss = 0\n",
    "            for batch_idx, (data, label) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "                loss = train_step(data, label)\n",
    "                epoch_loss += loss.asnumpy()\n",
    "\n",
    "                if np.isnan(loss.asnumpy()):\n",
    "                    print(f\"NaN detected at batch {batch_idx}\")\n",
    "                    break\n",
    "\n",
    "                if batch_idx % 100 == 0:\n",
    "                    processed_samples = batch_idx * len(data)\n",
    "                    print(\n",
    "                        f'Train Epoch: {epoch} [{processed_samples}/{total_train_samples} ({100. * batch_idx / batches_per_epoch:0.0f}%)]\\tLoss: {loss.asnumpy():.6f}')\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} completed, Average Loss: {epoch_loss / batches_per_epoch:.6f}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# 测试函数\n",
    "def test(net, test_dataset):\n",
    "    net.set_train(False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, label in test_dataset.create_tuple_iterator():\n",
    "        logits, _ = net(data)\n",
    "        pred = ops.Argmax(axis=1)(logits)\n",
    "        correct += (pred == label).asnumpy().sum()\n",
    "        total += len(label)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 数据集路径\n",
    "    data_path = r\"D:\\MindSporeProject\\pythonProject1\\Large_Margin_Loss_PyTorch\\data\"\n",
    "\n",
    "    # 加载训练集和测试集\n",
    "    train_dataset = create_dataset(data_path, usage=\"train\", batch_size=256)\n",
    "    test_dataset = create_dataset(data_path, usage=\"test\", batch_size=2048)\n",
    "\n",
    "    # 初始化网络、损失函数和优化器\n",
    "    net = Net()\n",
    "    loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')  # 使用交叉熵损失\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = nn.Adam(net.trainable_params(), learning_rate=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # 训练和测试\n",
    "    for epoch in range(5):\n",
    "        train(net, loss_fn, optimizer, train_dataset, epoch_size=1)\n",
    "        test(net, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19776:15908,MainProcess):2025-04-01-15:41:12.471.000 [mindspore\\context.py:1335] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n",
      "[WARNING] ME(19776:15908,MainProcess):2025-04-01-15:41:12.492.000 [mindspore\\nn\\layer\\basic.py:176] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/59904 (0%)]\tLoss: 0.059993\n",
      "Train Epoch: 0 [25600/59904 (43%)]\tLoss: 0.044115\n",
      "Train Epoch: 0 [51200/59904 (85%)]\tLoss: 0.034634\n",
      "Epoch 1 completed, Average Loss: 0.046905\n",
      "Test Accuracy: 66.72%\n",
      "Train Epoch: 1 [0/59904 (0%)]\tLoss: 0.036516\n",
      "Train Epoch: 1 [25600/59904 (43%)]\tLoss: 0.034723\n",
      "Train Epoch: 1 [51200/59904 (85%)]\tLoss: 0.033691\n",
      "Epoch 2 completed, Average Loss: 0.035024\n",
      "Test Accuracy: 72.80%\n",
      "Train Epoch: 2 [0/59904 (0%)]\tLoss: 0.035038\n",
      "Train Epoch: 2 [25600/59904 (43%)]\tLoss: 0.036096\n",
      "Train Epoch: 2 [51200/59904 (85%)]\tLoss: 0.031590\n",
      "Epoch 3 completed, Average Loss: 0.031155\n",
      "Test Accuracy: 75.28%\n",
      "Train Epoch: 3 [0/59904 (0%)]\tLoss: 0.031711\n",
      "Train Epoch: 3 [25600/59904 (43%)]\tLoss: 0.023508\n",
      "Train Epoch: 3 [51200/59904 (85%)]\tLoss: 0.025329\n",
      "Epoch 4 completed, Average Loss: 0.027380\n",
      "Test Accuracy: 76.33%\n",
      "Train Epoch: 4 [0/59904 (0%)]\tLoss: 0.025513\n",
      "Train Epoch: 4 [25600/59904 (43%)]\tLoss: 0.022840\n",
      "Train Epoch: 4 [51200/59904 (85%)]\tLoss: 0.025412\n",
      "Epoch 5 completed, Average Loss: 0.024723\n",
      "Test Accuracy: 65.60%\n",
      "Training completed. Best accuracy: 76.33%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor, context\n",
    "from mindspore.dataset import MnistDataset\n",
    "from mindspore.dataset.transforms import TypeCast\n",
    "from mindspore.dataset.vision import Rescale, HWC2CHW, RandomCrop, RandomHorizontalFlip\n",
    "from mindspore.train import Model\n",
    "from large_margin import LargeMarginLoss\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target=\"CPU\")\n",
    "\n",
    "\n",
    "# 数据加载函数\n",
    "def create_dataset(data_path, usage=\"train\", batch_size=256):\n",
    "    mnist_dataset = MnistDataset(dataset_dir=data_path, usage=usage, shuffle=True)\n",
    "    transform = [\n",
    "        Rescale(1.0 / 127.5, -1),\n",
    "        RandomCrop(28, padding=4),\n",
    "        RandomHorizontalFlip(prob=0.5),\n",
    "        HWC2CHW()\n",
    "    ]\n",
    "    mnist_dataset = mnist_dataset.map(transform, 'image')\n",
    "    mnist_dataset = mnist_dataset.map(TypeCast(ms.float32), 'image')\n",
    "    mnist_dataset = mnist_dataset.map(TypeCast(ms.int32), 'label')\n",
    "    mnist_dataset = mnist_dataset.batch(batch_size, drop_remainder=True)\n",
    "    return mnist_dataset\n",
    "\n",
    "# 定义网络\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, pad_mode='same')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, pad_mode='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = ops.Flatten()\n",
    "        self.fc1 = nn.Dense(64 * 7 * 7, 256)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Dense(256, 10)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        conv1 = x\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        conv2 = x\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return logits, [conv1, conv2]\n",
    "\n",
    "# 训练函数\n",
    "# 将 test 定义为独立函数\n",
    "def test(net, test_dataset):\n",
    "    net.set_train(False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, label in test_dataset.create_tuple_iterator():\n",
    "        logits, _ = net(data)\n",
    "        pred = ops.Argmax(axis=1)(logits)\n",
    "        correct += (pred == label).asnumpy().sum()\n",
    "        total += len(label)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_lm(net, train_dataset, test_dataset, optimizer, epoch_size=5):\n",
    "    lm_loss = LargeMarginLoss(gamma=10000, alpha_factor=4, top_k=1, epsilon=1e-6)\n",
    "\n",
    "    # 定义前向传播和梯度计算的函数\n",
    "    def forward_fn(data, label):\n",
    "        logits, feature_maps = net(data)\n",
    "        one_hot = Tensor(np.eye(10)[label.asnumpy()], ms.float32)\n",
    "        loss = lm_loss(logits, one_hot, feature_maps)\n",
    "        return loss\n",
    "\n",
    "    # 使用MindSpore的value_and_grad函数来获取梯度\n",
    "    grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "    # 定义单步训练函数\n",
    "    def train_step(data, label):\n",
    "        loss, grads = grad_fn(data, label)\n",
    "        optimizer(grads)  # MindSpore中优化器直接用来更新参数\n",
    "        return loss\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        net.set_train(True)\n",
    "        epoch_loss = 0\n",
    "        total_batches = len(train_dataset)  # 获取批次总数\n",
    "\n",
    "        for batch_idx, (data, label) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "            loss = train_step(data, label)\n",
    "            epoch_loss += loss.asnumpy()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                batch_size = data.shape[0]  # 从当前批次数据获取批次大小\n",
    "                processed_samples = batch_idx * batch_size\n",
    "                print(\n",
    "                    f'Train Epoch: {epoch} [{processed_samples}/{total_batches * batch_size} ({100. * batch_idx / total_batches:.0f}%)]\\tLoss: {loss.asnumpy():.6f}')\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed, Average Loss: {epoch_loss / total_batches:.6f}\")\n",
    "\n",
    "        # 在每个周期后评估模型\n",
    "        accuracy = test(net, test_dataset)\n",
    "\n",
    "        # 跟踪最佳准确率\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    print(f\"Training completed. Best accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 数据集路径\n",
    "    data_path = r\"D:\\MindSporeProject\\pythonProject1\\Large_Margin_Loss_PyTorch\\data\"\n",
    "\n",
    "    # 加载训练集和测试集\n",
    "    train_dataset = create_dataset(data_path, usage=\"train\", batch_size=256)\n",
    "    test_dataset = create_dataset(data_path, usage=\"test\", batch_size=2048)\n",
    "\n",
    "    # 初始化网络和优化器\n",
    "    net = Net()\n",
    "    optimizer = nn.Adam(net.trainable_params(), learning_rate=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # 训练和测试\n",
    "    train_lm(net, train_dataset, test_dataset, optimizer, epoch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
